# Machine-Translation-using-Attention-Mechanism
Implementing sequence to sequence(seq2seq) neural machine translation(NMT) for English to Deutch using Bahdanauâ€™s Attention mechanism. We will implement the code in Tensorflow 2.0 using Gated Recurrent Unit(GRU).

# Goal
In this project,we build a deep neural network that functions as part of a machine translation pipeline. The pipeline accepts English text as input and returns the Deutch(German) translation. The goal is to achieve the highest translation accuracy possible using state of the art techniques.

# Approach
To translate a corpus of English text to Deutch, we will be using the Bahadanau's attention model architecture.
For implementation purposes, we will use English as the source language and Deutch(German) as the target language.
The code will be implemented using TensorFlow 2.0, and data can be downloaded from [here](http://www.manythings.org/anki/).

# Steps
1. Preprocessing
   - Data loading
   - Data cleaning
   - Tokenization 
   - Padding
2. Modelling/Training the dataset using Encoder-Decoder model
3. Predictions

# Results
